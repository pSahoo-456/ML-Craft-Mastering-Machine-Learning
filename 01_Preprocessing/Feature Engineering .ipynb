{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aa2960d",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Feature engineering is the process of transforming raw data into features that better represent the underlying problem to predictive models, resulting in improved model accuracy on unseen data. It is one of the most important steps in the machine learning pipeline.\n",
    "\n",
    "In this notebook, we'll explore various feature engineering techniques that can significantly improve your model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd2adaf",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fad7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d56653",
   "metadata": {},
   "source": [
    "## Sample Dataset\n",
    "\n",
    "Let's create a sample dataset to demonstrate various feature engineering techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3021466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate synthetic data\n",
    "age = np.random.randint(18, 80, n_samples)\n",
    "income = np.random.normal(50000, 15000, n_samples)\n",
    "experience = np.random.randint(0, 40, n_samples)\n",
    "education_years = np.random.randint(8, 20, n_samples)\n",
    "\n",
    "# Create some relationships\n",
    "purchase_amount = (\n",
    "    0.5 * age +\n",
    "    0.0001 * income +\n",
    "    2 * experience +\n",
    "    10 * education_years +\n",
    "    np.random.normal(0, 100, n_samples)  # noise\n",
    ")\n",
    "\n",
    "# Ensure positive values\n",
    "purchase_amount = np.abs(purchase_amount)\n",
    "\n",
    "# Create categorical features\n",
    "cities = np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'], n_samples)\n",
    "membership = np.random.choice(['Basic', 'Premium', 'VIP'], n_samples, p=[0.6, 0.3, 0.1])\n",
    "\n",
    "# Create DataFrame\n",
    "data = {\n",
    "    'age': age,\n",
    "    'income': income,\n",
    "    'experience': experience,\n",
    "    'education_years': education_years,\n",
    "    'city': cities,\n",
    "    'membership': membership,\n",
    "    'purchase_amount': purchase_amount\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original Dataset:\")\n",
    "print(df.head(10))\n",
    "print(\"\\nDataset Shape:\", df.shape)\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ec07ec",
   "metadata": {},
   "source": [
    "## 1. Mathematical Transformations\n",
    "\n",
    "Mathematical transformations can help normalize distributions, stabilize variance, or create new meaningful features.\n",
    "\n",
    "**Common transformations:**\n",
    "- Logarithmic transformation\n",
    "- Square root transformation\n",
    "- Box-Cox transformation\n",
    "- Polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dd5313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logarithmic transformation\n",
    "df['log_income'] = np.log1p(df['income'])  # log(1+x) to handle zero values\n",
    "\n",
    "# Square root transformation\n",
    "df['sqrt_income'] = np.sqrt(df['income'])\n",
    "\n",
    "# Polynomial features (example with age)\n",
    "df['age_squared'] = df['age'] ** 2\n",
    "df['age_cubed'] = df['age'] ** 3\n",
    "\n",
    "print(\"Dataset after mathematical transformations:\")\n",
    "print(df[['income', 'log_income', 'sqrt_income', 'age', 'age_squared', 'age_cubed']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b740841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of transformations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Original income distribution\n",
    "axes[0, 0].hist(df['income'], bins=30, alpha=0.7)\n",
    "axes[0, 0].set_title('Original Income Distribution')\n",
    "axes[0, 0].set_xlabel('Income')\n",
    "\n",
    "# Log-transformed income\n",
    "axes[0, 1].hist(df['log_income'], bins=30, alpha=0.7, color='orange')\n",
    "axes[0, 1].set_title('Log-Transformed Income Distribution')\n",
    "axes[0, 1].set_xlabel('Log(Income)')\n",
    "\n",
    "# Square root transformed income\n",
    "axes[1, 0].hist(df['sqrt_income'], bins=30, alpha=0.7, color='green')\n",
    "axes[1, 0].set_title('Square Root Transformed Income')\n",
    "axes[1, 0].set_xlabel('Sqrt(Income)')\n",
    "\n",
    "# Age squared\n",
    "axes[1, 1].hist(df['age_squared'], bins=30, alpha=0.7, color='red')\n",
    "axes[1, 1].set_title('Age Squared Distribution')\n",
    "axes[1, 1].set_xlabel('Age²')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2ec702",
   "metadata": {},
   "source": [
    "## 2. Binning (Discretization)\n",
    "\n",
    "Binning converts continuous variables into categorical ones by grouping values into bins. This can help capture non-linear relationships.\n",
    "\n",
    "**Types of binning:**\n",
    "- Equal-width binning\n",
    "- Equal-frequency binning\n",
    "- Custom binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c216bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equal-width binning\n",
    "df['age_binned_equal_width'] = pd.cut(df['age'], bins=5, labels=['Very Young', 'Young', 'Middle', 'Old', 'Very Old'])\n",
    "\n",
    "# Equal-frequency binning (quantile binning)\n",
    "df['income_binned_quantile'] = pd.qcut(df['income'], q=5, labels=['Q1', 'Q2', 'Q3', 'Q4', 'Q5'])\n",
    "\n",
    "# Custom binning\n",
    "income_bins = [0, 30000, 50000, 70000, 100000, np.inf]\n",
    "income_labels = ['Low', 'Lower-Middle', 'Middle', 'Upper-Middle', 'High']\n",
    "df['income_binned_custom'] = pd.cut(df['income'], bins=income_bins, labels=income_labels)\n",
    "\n",
    "print(\"Binned features:\")\n",
    "print(df[['age', 'age_binned_equal_width', 'income', 'income_binned_quantile', 'income_binned_custom']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28879255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize binning results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Age distribution with equal-width bins\n",
    "df['age_binned_equal_width'].value_counts().plot(kind='bar', ax=axes[0], color='skyblue')\n",
    "axes[0].set_title('Age Distribution (Equal-Width Bins)')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Income distribution with quantile bins\n",
    "df['income_binned_quantile'].value_counts().plot(kind='bar', ax=axes[1], color='lightgreen')\n",
    "axes[1].set_title('Income Distribution (Quantile Bins)')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Income distribution with custom bins\n",
    "df['income_binned_custom'].value_counts().plot(kind='bar', ax=axes[2], color='salmon')\n",
    "axes[2].set_title('Income Distribution (Custom Bins)')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99667cfc",
   "metadata": {},
   "source": [
    "## 3. Interaction Features\n",
    "\n",
    "Interaction features combine two or more features to capture relationships between them. These can reveal hidden patterns in the data.\n",
    "\n",
    "**Common interactions:**\n",
    "- Multiplication of features\n",
    "- Ratio features\n",
    "- Polynomial combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21afa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplication interactions\n",
    "df['income_age_interaction'] = df['income'] * df['age']\n",
    "df['experience_education_interaction'] = df['experience'] * df['education_years']\n",
    "\n",
    "# Ratio features\n",
    "df['income_per_experience'] = df['income'] / (df['experience'] + 1)  # Adding 1 to avoid division by zero\n",
    "df['education_per_age'] = df['education_years'] / df['age']\n",
    "\n",
    "# Polynomial features using sklearn\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
    "features_for_poly = df[['age', 'income', 'experience']].head(5)  # Using subset for demonstration\n",
    "poly_features_matrix = poly_features.fit_transform(features_for_poly)\n",
    "\n",
    "print(\"Original features:\")\n",
    "print(features_for_poly)\n",
    "print(\"\\nPolynomial interaction features (degree=2, interaction only):\")\n",
    "print(poly_features_matrix)\n",
    "print(\"\\nFeature names:\")\n",
    "print(poly_features.get_feature_names_out(['age', 'income', 'experience']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4462ef85",
   "metadata": {},
   "source": [
    "## 4. Date and Time Features\n",
    "\n",
    "When working with temporal data, extracting meaningful features from dates and times can significantly improve model performance.\n",
    "\n",
    "**Common date/time features:**\n",
    "- Year, month, day, hour\n",
    "- Day of week, weekend indicator\n",
    "- Seasonal indicators\n",
    "- Time since a reference point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353b08a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample date data\n",
    "date_range = pd.date_range(start='2020-01-01', periods=n_samples, freq='D')\n",
    "df_dates = pd.DataFrame({'date': date_range})\n",
    "\n",
    "# Extract date components\n",
    "df_dates['year'] = df_dates['date'].dt.year\n",
    "df_dates['month'] = df_dates['date'].dt.month\n",
    "df_dates['day'] = df_dates['date'].dt.day\n",
    "df_dates['day_of_week'] = df_dates['date'].dt.dayofweek\n",
    "df_dates['day_name'] = df_dates['date'].dt.day_name()\n",
    "df_dates['quarter'] = df_dates['date'].dt.quarter\n",
    "df_dates['is_weekend'] = df_dates['day_of_week'].isin([5, 6]).astype(int)  # 5=Saturday, 6=Sunday\n",
    "\n",
    "print(\"Date feature extraction:\")\n",
    "print(df_dates.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764fc373",
   "metadata": {},
   "source": [
    "## 5. Domain-Specific Features\n",
    "\n",
    "Domain knowledge can help create highly predictive features. These are specific to the problem domain.\n",
    "\n",
    "**Examples:**\n",
    "- BMI from height and weight\n",
    "- Customer lifetime value\n",
    "- Price-to-income ratio\n",
    "- Efficiency metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b0bfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain-specific features based on our dataset\n",
    "\n",
    "# Income efficiency (income per year of education)\n",
    "df['income_efficiency'] = df['income'] / df['education_years']\n",
    "\n",
    "# Experience ratio (experience relative to age)\n",
    "df['experience_ratio'] = df['experience'] / df['age']\n",
    "\n",
    "# Income category based on domain knowledge\n",
    "def categorize_income(income):\n",
    "    if income < 30000:\n",
    "        return 'Low'\n",
    "    elif income < 50000:\n",
    "        return 'Lower-Middle'\n",
    "    elif income < 70000:\n",
    "        return 'Middle'\n",
    "    elif income < 100000:\n",
    "        return 'Upper-Middle'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "df['income_category'] = df['income'].apply(categorize_income)\n",
    "\n",
    "print(\"Domain-specific features:\")\n",
    "print(df[['income', 'education_years', 'income_efficiency', 'experience', 'age', 'experience_ratio', 'income_category']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb702f71",
   "metadata": {},
   "source": [
    "## 6. Feature Scaling\n",
    "\n",
    "Scaling features to a similar range is crucial for many machine learning algorithms. Different scaling techniques serve different purposes.\n",
    "\n",
    "**Common scaling methods:**\n",
    "- Standardization (Z-score normalization)\n",
    "- Min-Max scaling\n",
    "- Robust scaling\n",
    "- Unit vector scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77475d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical features for scaling\n",
    "numerical_features = ['age', 'income', 'experience', 'education_years']\n",
    "df_numerical = df[numerical_features].copy()\n",
    "\n",
    "# Standardization (Z-score normalization)\n",
    "scaler_standard = StandardScaler()\n",
    "df_scaled_standard = scaler_standard.fit_transform(df_numerical)\n",
    "df_scaled_standard = pd.DataFrame(df_scaled_standard, columns=[f'{col}_standard' for col in numerical_features])\n",
    "\n",
    "# Min-Max scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler_minmax = MinMaxScaler()\n",
    "df_scaled_minmax = scaler_minmax.fit_transform(df_numerical)\n",
    "df_scaled_minmax = pd.DataFrame(df_scaled_minmax, columns=[f'{col}_minmax' for col in numerical_features])\n",
    "\n",
    "print(\"Original numerical features:\")\n",
    "print(df_numerical.describe())\n",
    "print(\"\\nStandardized features (mean≈0, std≈1):\")\n",
    "print(df_scaled_standard.describe())\n",
    "print(\"\\nMin-Max scaled features (range 0-1):\")\n",
    "print(df_scaled_minmax.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c2aebb",
   "metadata": {},
   "source": [
    "## 7. Feature Selection\n",
    "\n",
    "Not all features contribute equally to model performance. Feature selection helps identify the most relevant features.\n",
    "\n",
    "**Common methods:**\n",
    "- Correlation analysis\n",
    "- Variance threshold\n",
    "- Recursive feature elimination\n",
    "- Feature importance from tree-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7d4e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "correlation_matrix = df[['age', 'income', 'experience', 'education_years', 'purchase_amount']].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(\"Correlation with target variable (purchase_amount):\")\n",
    "target_corr = correlation_matrix['purchase_amount'].drop('purchase_amount').sort_values(key=abs, ascending=False)\n",
    "print(target_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef4d390",
   "metadata": {},
   "source": [
    "## 8. Putting It All Together: A Complete Example\n",
    "\n",
    "Let's apply feature engineering to improve a machine learning model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69337125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a complete example with feature engineering\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Prepare the dataset\n",
    "df_model = df.copy()\n",
    "\n",
    "# Apply feature engineering\n",
    "# 1. Mathematical transformations\n",
    "df_model['log_income'] = np.log1p(df_model['income'])\n",
    "df_model['age_squared'] = df_model['age'] ** 2\n",
    "\n",
    "# 2. Binning\n",
    "df_model['income_category'] = df_model['income'].apply(categorize_income)\n",
    "\n",
    "# 3. Interaction features\n",
    "df_model['income_age_interaction'] = df_model['income'] * df_model['age']\n",
    "df_model['experience_education_interaction'] = df_model['experience'] * df_model['education_years']\n",
    "\n",
    "# 4. Domain-specific features\n",
    "df_model['income_efficiency'] = df_model['income'] / df_model['education_years']\n",
    "df_model['experience_ratio'] = df_model['experience'] / df_model['age']\n",
    "\n",
    "# Encode categorical variables\n",
    "df_model = pd.get_dummies(df_model, columns=['city', 'membership', 'income_category'], prefix=['city', 'membership', 'income_cat'])\n",
    "\n",
    "# Split the data\n",
    "features = [col for col in df_model.columns if col != 'purchase_amount']\n",
    "X = df_model[features]\n",
    "y = df_model['purchase_amount']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train models with and without feature engineering\n",
    "# Baseline model (original features only)\n",
    "original_features = ['age', 'income', 'experience', 'education_years']\n",
    "X_train_baseline = X_train[original_features]\n",
    "X_test_baseline = X_test[original_features]\n",
    "\n",
    "model_baseline = LinearRegression()\n",
    "model_baseline.fit(X_train_baseline, y_train)\n",
    "y_pred_baseline = model_baseline.predict(X_test_baseline)\n",
    "\n",
    "# Enhanced model (with feature engineering)\n",
    "model_enhanced = LinearRegression()\n",
    "model_enhanced.fit(X_train, y_train)\n",
    "y_pred_enhanced = model_enhanced.predict(X_test)\n",
    "\n",
    "# Compare performance\n",
    "mse_baseline = mean_squared_error(y_test, y_pred_baseline)\n",
    "mse_enhanced = mean_squared_error(y_test, y_pred_enhanced)\n",
    "\n",
    "r2_baseline = r2_score(y_test, y_pred_baseline)\n",
    "r2_enhanced = r2_score(y_test, y_pred_enhanced)\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(f\"Baseline Model (Original Features Only):\")\n",
    "print(f\"  MSE: {mse_baseline:.2f}\")\n",
    "print(f\"  R²: {r2_baseline:.4f}\")\n",
    "print(f\"\\nEnhanced Model (With Feature Engineering):\")\n",
    "print(f\"  MSE: {mse_enhanced:.2f}\")\n",
    "print(f\"  R²: {r2_enhanced:.4f}\")\n",
    "print(f\"\\nImprovement in R²: {r2_enhanced - r2_baseline:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fc65a0",
   "metadata": {},
   "source": [
    "## 9. Using Pipelines for Feature Engineering\n",
    "\n",
    "Pipelines help organize and automate feature engineering steps, making the process reproducible and preventing data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8717f4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Define feature engineering pipeline\n",
    "# Numerical features pipeline\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical features pipeline\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine pipelines\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, ['age', 'income', 'experience', 'education_years']),\n",
    "        ('cat', categorical_pipeline, ['city', 'membership'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Complete pipeline\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Prepare data for pipeline\n",
    "X_pipeline = df[['age', 'income', 'experience', 'education_years', 'city', 'membership']]\n",
    "y_pipeline = df['purchase_amount']\n",
    "\n",
    "X_train_pipe, X_test_pipe, y_train_pipe, y_test_pipe = train_test_split(\n",
    "    X_pipeline, y_pipeline, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Fit and evaluate pipeline\n",
    "full_pipeline.fit(X_train_pipe, y_train_pipe)\n",
    "y_pred_pipe = full_pipeline.predict(X_test_pipe)\n",
    "\n",
    "mse_pipeline = mean_squared_error(y_test_pipe, y_pred_pipe)\n",
    "r2_pipeline = r2_score(y_test_pipe, y_pred_pipe)\n",
    "\n",
    "print(\"Pipeline Model Performance:\")\n",
    "print(f\"  MSE: {mse_pipeline:.2f}\")\n",
    "print(f\"  R²: {r2_pipeline:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b05ba1",
   "metadata": {},
   "source": [
    "## Feature Engineering Best Practices\n",
    "\n",
    "| Practice | Description | Benefits |\n",
    "|----------|-------------|----------|\n",
    "| Understand your data | Explore distributions, relationships, and anomalies | Better feature creation |\n",
    "| Use domain knowledge | Leverage expertise to create meaningful features | Higher predictive power |\n",
    "| Validate your features | Test if new features improve model performance | Avoid useless features |\n",
    "| Prevent data leakage | Don't use future information in training | Reliable model performance |\n",
    "| Automate with pipelines | Create reproducible feature engineering workflows | Consistency and efficiency |\n",
    "| Monitor feature importance | Track which features contribute most | Model interpretability |\n",
    "\n",
    "**Key Guidelines:**\n",
    "1. **Start simple**: Begin with basic features and gradually add complexity\n",
    "2. **Validate everything**: Always test if a new feature improves performance\n",
    "3. **Avoid overfitting**: Don't create too many features relative to your sample size\n",
    "4. **Document your process**: Keep track of what works and what doesn't\n",
    "5. **Think causally**: Consider whether a feature might actually influence the target\n",
    "6. **Consider computational cost**: Balance feature complexity with model training time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432d8c8e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Feature engineering is both an art and a science that requires creativity, domain knowledge, and experimentation. The techniques we've covered include:\n",
    "\n",
    "1. **Mathematical transformations** to normalize distributions and create polynomial features\n",
    "2. **Binning** to convert continuous variables into categorical ones\n",
    "3. **Interaction features** to capture relationships between variables\n",
    "4. **Date/time features** for temporal data\n",
    "5. **Domain-specific features** based on expert knowledge\n",
    "6. **Feature scaling** to prepare data for algorithms\n",
    "7. **Feature selection** to identify the most relevant features\n",
    "8. **Pipelines** to automate and organize the process\n",
    "\n",
    "Remember that feature engineering is an iterative process. Start with simple transformations, validate their impact, and gradually build more complex features. The goal is to create features that help your model better understand the underlying patterns in your data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
